{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EGqrrXzv2-Cn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3513f647-e544-4688-9342-31f7e4597dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 126897 files and directories currently installed.)\n",
            "Preparing to unpack chrome.deb ...\n",
            "Unpacking google-chrome-stable (138.0.7204.183-1) over (138.0.7204.183-1) ...\n",
            "Setting up google-chrome-stable (138.0.7204.183-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "âœ… Success! Page title: Example Domain\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import dateutil\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import urllib3\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
        "# ğŸ“¦ Install Google Chrome\n",
        "!wget -q -O chrome.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i chrome.deb || apt-get -fy install > /dev/null\n",
        "\n",
        "# ğŸ“¥ Get matching ChromeDriver\n",
        "import os, re, requests, zipfile, io\n",
        "\n",
        "# Get Chrome version\n",
        "version_output = !google-chrome --version\n",
        "major_version = re.search(r'(\\d+)\\.', version_output[0]).group(1)\n",
        "\n",
        "# Get matching ChromeDriver URL\n",
        "resp = requests.get('https://googlechromelabs.github.io/chrome-for-testing/last-known-good-versions-with-downloads.json')\n",
        "driver_info = resp.json()['channels']['Stable']['downloads']['chromedriver']\n",
        "driver_url = next(d['url'] for d in driver_info if d['platform'] == 'linux64')\n",
        "\n",
        "# Download and extract ChromeDriver\n",
        "driver_zip = requests.get(driver_url)\n",
        "z = zipfile.ZipFile(io.BytesIO(driver_zip.content))\n",
        "extract_path = \"/usr/bin/chromedriver-linux64\"\n",
        "z.extractall(extract_path)\n",
        "os.chmod(f\"{extract_path}/chromedriver\", 0o755)\n",
        "\n",
        "# ğŸ”„ Fix symlink safely\n",
        "if os.path.islink(\"/usr/bin/chromedriver\") or os.path.exists(\"/usr/bin/chromedriver\"):\n",
        "    os.remove(\"/usr/bin/chromedriver\")\n",
        "os.symlink(f\"{extract_path}/chromedriver\", \"/usr/bin/chromedriver\")\n",
        "\n",
        "# âœ… Install Python packages\n",
        "!pip install -q selenium beautifulsoup4\n",
        "\n",
        "# âœ… Test it works\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "driver = webdriver.Chrome(service=Service(\"/usr/bin/chromedriver\"), options=chrome_options)\n",
        "driver.get(\"https://example.com\")\n",
        "print(\"âœ… Success! Page title:\", driver.title)\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('NY_TIMES_KEY')"
      ],
      "metadata": {
        "id": "h3kC2_4w3JQq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def send_request(date):\n",
        "    base_url = 'https://api.nytimes.com/svc/archive/v1/'\n",
        "    url = base_url + '/' + date[0] + '/' + date[1] + '.json?api-key=' + API_KEY\n",
        "    try:\n",
        "        response = requests.get(url, verify=False).json()\n",
        "    except Exception:\n",
        "        return None\n",
        "    time.sleep(6)\n",
        "    return response\n",
        "\n",
        "\n",
        "def is_valid(article, date):\n",
        "    is_in_range = date > start and date < end\n",
        "    has_headline = type(article['headline']) == dict and 'main' in article['headline'].keys()\n",
        "    return is_in_range and has_headline\n",
        "\n",
        "\n",
        "def parse_response(response):\n",
        "    data = {'headline': [],\n",
        "        'date': [],\n",
        "        'web_url': [],\n",
        "        'doc_type': [],\n",
        "        'lead_paragraph': [],\n",
        "        'material_type': [],\n",
        "        'author': [],\n",
        "        'section': [],\n",
        "        'subsection': [],\n",
        "        'keywords': []}\n",
        "\n",
        "    articles = response['response']['docs']\n",
        "    for article in articles:\n",
        "        date = dateutil.parser.parse(article['pub_date']).date()\n",
        "        if is_valid(article, date):\n",
        "            data['date'].append(date)\n",
        "            data['headline'].append(article['headline']['main'])\n",
        "            if 'section_name' in article:\n",
        "                data['section'].append(article['section_name'])\n",
        "            else:\n",
        "                data['section'].append(None)\n",
        "            if 'lead_paragraph' in article:\n",
        "                data['lead_paragraph'].append(article['lead_paragraph'])\n",
        "            else:\n",
        "                data['lead_paragraph'].append(None)\n",
        "            if 'web_url' in article:\n",
        "                data['web_url'].append(article['web_url'])\n",
        "            else:\n",
        "                data['web_url'].append(None)\n",
        "            if 'subsection_name' in article:\n",
        "                data['subsection'].append(article['subsection_name'])\n",
        "            else:\n",
        "                data['subsection'].append(None)\n",
        "            if 'byline' in article:\n",
        "                data['author'].append(article['byline']['original'])\n",
        "            else:\n",
        "                data['author'].append(None)\n",
        "            data['doc_type'].append(article['document_type'])\n",
        "            if 'type_of_material' in article:\n",
        "                data['material_type'].append(article['type_of_material'])\n",
        "            else:\n",
        "                data['material_type'].append(None)\n",
        "            keywords = [keyword['value'] for keyword in article['keywords'] if keyword['name'] == 'subject']\n",
        "            data['keywords'].append(keywords)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "def get_data(dates):\n",
        "    total = 0\n",
        "    print('Date range: ' + str(dates[0]) + ' to ' + str(dates[-1]))\n",
        "    if not os.path.exists('headlines'):\n",
        "        os.mkdir('headlines')\n",
        "    for date in dates:\n",
        "        print('Working on ' + str(date) + '...')\n",
        "        csv_path = 'headlines/' + date[0] + '-' + date[1] + '.csv'\n",
        "        if not os.path.exists(csv_path): # If we don't already have this month\n",
        "            response = send_request(date)\n",
        "            if response is not None:\n",
        "                df = parse_response(response)\n",
        "                total += len(df)\n",
        "                df.to_csv(csv_path, index=False)\n",
        "                print('Saving ' + csv_path + '...')\n",
        "    print('Number of articles collected: ' + str(total))"
      ],
      "metadata": {
        "id": "aSDRXhSU3JS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "end = datetime.date(2020, 12, 31)\n",
        "start = datetime.date(2000, 1, 1)"
      ],
      "metadata": {
        "id": "xfHkU-BT3JVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "months = [x.split(' ') for x in pd.date_range(start, end, freq='MS').strftime(\"%Y %-m\").tolist()]"
      ],
      "metadata": {
        "id": "6MuDrhq-3diI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_data(months)"
      ],
      "metadata": {
        "id": "F-RIq3oM3dn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "os.chdir(\"/content/headlines\") ## use Google Colab"
      ],
      "metadata": {
        "id": "RE-EB51o3duw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extension = 'csv'\n",
        "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]"
      ],
      "metadata": {
        "id": "VW_gz-m13rcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine in a single file\n",
        "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
        "#export to csv\n",
        "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "n1DDqwUD3td0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}